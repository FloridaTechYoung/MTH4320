{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [10 points] Consider a dense neural network classifier with 2D input, 1 hidden layer with 3 neurons with ReLU activation, and 3D output with softmax. Generate random numbers for the weights and compute the output for $(4,5)$\n",
    ". Then, compute the gradient with respect to the weights using backpropagation for a MSE loss. (Paper and pencil question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [30 points] Use a feedforward NN to classify the CIFAR-10 dataset, and tune its hyperparameters as best you can. You must use PyTorch. Requirements below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                 [-1, 3072]               0\n",
      "            Linear-2                  [-1, 256]         786,688\n",
      "              ReLU-3                  [-1, 256]               0\n",
      "           Dropout-4                  [-1, 256]               0\n",
      "            Linear-5                  [-1, 128]          32,896\n",
      "              ReLU-6                  [-1, 128]               0\n",
      "            Linear-7                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 820,874\n",
      "Trainable params: 820,874\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.03\n",
      "Params size (MB): 3.13\n",
      "Estimated Total Size (MB): 3.17\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchsummary import summary\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "import torch.nn.init as init\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random_state = 1\n",
    "torch.manual_seed(random_state)\n",
    "\n",
    "# CIFAR-10 dataset preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))  # Normalization for CIFAR-10\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split dataset into 60/20/20\n",
    "train_size = int(0.6 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# single 10-node hidden layer\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(32*32*3, 256),  # Input size is for CIFAIR10 is 3072 (32x32x3)\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.1),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Use CrossEntropyLoss for classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam optimizer with L2 regularization (weight_decay is the L2 penalty)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003, weight_decay=1e-4)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "summary(model, input_size=(3072,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: \tTrain Loss: 1.7898 \tTest Loss: 1.6404\tTrain Accuracy: 0.3670, \tTest Accuracy: 0.4125\n",
      "Epoch 1: \tTrain Loss: 1.5670 \tTest Loss: 1.5561\tTrain Accuracy: 0.4473, \tTest Accuracy: 0.4546\n",
      "Epoch 2: \tTrain Loss: 1.4888 \tTest Loss: 1.5263\tTrain Accuracy: 0.4773, \tTest Accuracy: 0.4633\n",
      "Epoch 3: \tTrain Loss: 1.4294 \tTest Loss: 1.5157\tTrain Accuracy: 0.4948, \tTest Accuracy: 0.4688\n",
      "Epoch 4: \tTrain Loss: 1.3875 \tTest Loss: 1.5471\tTrain Accuracy: 0.5096, \tTest Accuracy: 0.4700\n",
      "Epoch 5: \tTrain Loss: 1.3558 \tTest Loss: 1.5085\tTrain Accuracy: 0.5225, \tTest Accuracy: 0.4805\n",
      "Epoch 6: \tTrain Loss: 1.3283 \tTest Loss: 1.5035\tTrain Accuracy: 0.5317, \tTest Accuracy: 0.4778\n",
      "Epoch 7: \tTrain Loss: 1.2931 \tTest Loss: 1.5045\tTrain Accuracy: 0.5407, \tTest Accuracy: 0.4784\n",
      "Epoch 8: \tTrain Loss: 1.2673 \tTest Loss: 1.4862\tTrain Accuracy: 0.5548, \tTest Accuracy: 0.4816\n",
      "Epoch 9: \tTrain Loss: 1.2370 \tTest Loss: 1.4941\tTrain Accuracy: 0.5607, \tTest Accuracy: 0.4893\n",
      "Epoch 10: \tTrain Loss: 1.2141 \tTest Loss: 1.5071\tTrain Accuracy: 0.5742, \tTest Accuracy: 0.4879\n",
      "Epoch 11: \tTrain Loss: 1.2003 \tTest Loss: 1.5329\tTrain Accuracy: 0.5795, \tTest Accuracy: 0.4869\n",
      "Epoch 12: \tTrain Loss: 1.1888 \tTest Loss: 1.5231\tTrain Accuracy: 0.5795, \tTest Accuracy: 0.4886\n",
      "Epoch 13: \tTrain Loss: 1.1623 \tTest Loss: 1.5234\tTrain Accuracy: 0.5884, \tTest Accuracy: 0.4867\n",
      "Epoch 14: \tTrain Loss: 1.1509 \tTest Loss: 1.4927\tTrain Accuracy: 0.5925, \tTest Accuracy: 0.5002\n",
      "Epoch 15: \tTrain Loss: 1.1400 \tTest Loss: 1.5939\tTrain Accuracy: 0.5978, \tTest Accuracy: 0.4654\n",
      "Epoch 16: \tTrain Loss: 1.1175 \tTest Loss: 1.5332\tTrain Accuracy: 0.6048, \tTest Accuracy: 0.4906\n",
      "Epoch 17: \tTrain Loss: 1.1139 \tTest Loss: 1.5107\tTrain Accuracy: 0.6078, \tTest Accuracy: 0.4920\n",
      "Epoch 18: \tTrain Loss: 1.0873 \tTest Loss: 1.5213\tTrain Accuracy: 0.6156, \tTest Accuracy: 0.5019\n",
      "Epoch 19: \tTrain Loss: 1.0843 \tTest Loss: 1.5438\tTrain Accuracy: 0.6213, \tTest Accuracy: 0.4917\n",
      "Epoch 20: \tTrain Loss: 1.0828 \tTest Loss: 1.5517\tTrain Accuracy: 0.6183, \tTest Accuracy: 0.4892\n",
      "Epoch 21: \tTrain Loss: 1.0520 \tTest Loss: 1.5322\tTrain Accuracy: 0.6330, \tTest Accuracy: 0.4987\n",
      "Epoch 22: \tTrain Loss: 1.0434 \tTest Loss: 1.5144\tTrain Accuracy: 0.6343, \tTest Accuracy: 0.4857\n",
      "Epoch 23: \tTrain Loss: 1.0428 \tTest Loss: 1.5766\tTrain Accuracy: 0.6306, \tTest Accuracy: 0.4939\n",
      "Epoch 24: \tTrain Loss: 1.0397 \tTest Loss: 1.5373\tTrain Accuracy: 0.6358, \tTest Accuracy: 0.4909\n",
      "Epoch 25: \tTrain Loss: 1.0097 \tTest Loss: 1.5417\tTrain Accuracy: 0.6445, \tTest Accuracy: 0.4942\n",
      "Epoch 26: \tTrain Loss: 1.0065 \tTest Loss: 1.5903\tTrain Accuracy: 0.6458, \tTest Accuracy: 0.4999\n",
      "Epoch 27: \tTrain Loss: 0.9926 \tTest Loss: 1.5504\tTrain Accuracy: 0.6514, \tTest Accuracy: 0.4958\n",
      "Epoch 28: \tTrain Loss: 0.9961 \tTest Loss: 1.6190\tTrain Accuracy: 0.6510, \tTest Accuracy: 0.4720\n",
      "Epoch 29: \tTrain Loss: 0.9811 \tTest Loss: 1.5607\tTrain Accuracy: 0.6536, \tTest Accuracy: 0.5055\n",
      "Epoch 30: \tTrain Loss: 0.9781 \tTest Loss: 1.5715\tTrain Accuracy: 0.6556, \tTest Accuracy: 0.4911\n",
      "Epoch 31: \tTrain Loss: 0.9625 \tTest Loss: 1.5726\tTrain Accuracy: 0.6607, \tTest Accuracy: 0.4868\n",
      "Epoch 32: \tTrain Loss: 0.9638 \tTest Loss: 1.6324\tTrain Accuracy: 0.6594, \tTest Accuracy: 0.4856\n",
      "Epoch 33: \tTrain Loss: 0.9598 \tTest Loss: 1.6101\tTrain Accuracy: 0.6618, \tTest Accuracy: 0.4915\n",
      "Epoch 34: \tTrain Loss: 0.9557 \tTest Loss: 1.5978\tTrain Accuracy: 0.6655, \tTest Accuracy: 0.4938\n",
      "Epoch 35: \tTrain Loss: 0.9436 \tTest Loss: 1.5838\tTrain Accuracy: 0.6679, \tTest Accuracy: 0.4958\n",
      "Epoch 36: \tTrain Loss: 0.9396 \tTest Loss: 1.6232\tTrain Accuracy: 0.6717, \tTest Accuracy: 0.4997\n",
      "Epoch 37: \tTrain Loss: 0.9292 \tTest Loss: 1.5981\tTrain Accuracy: 0.6735, \tTest Accuracy: 0.4983\n",
      "Epoch 38: \tTrain Loss: 0.9204 \tTest Loss: 1.6166\tTrain Accuracy: 0.6788, \tTest Accuracy: 0.4954\n",
      "Epoch 39: \tTrain Loss: 0.9307 \tTest Loss: 1.6047\tTrain Accuracy: 0.6742, \tTest Accuracy: 0.4961\n",
      "Epoch 40: \tTrain Loss: 0.8953 \tTest Loss: 1.5967\tTrain Accuracy: 0.6860, \tTest Accuracy: 0.4988\n",
      "Epoch 41: \tTrain Loss: 0.8986 \tTest Loss: 1.6116\tTrain Accuracy: 0.6864, \tTest Accuracy: 0.4999\n",
      "Epoch 42: \tTrain Loss: 0.8941 \tTest Loss: 1.6499\tTrain Accuracy: 0.6846, \tTest Accuracy: 0.4786\n",
      "Epoch 43: \tTrain Loss: 0.8865 \tTest Loss: 1.6304\tTrain Accuracy: 0.6904, \tTest Accuracy: 0.4923\n",
      "Epoch 44: \tTrain Loss: 0.8832 \tTest Loss: 1.6478\tTrain Accuracy: 0.6927, \tTest Accuracy: 0.4901\n",
      "Epoch 45: \tTrain Loss: 0.8865 \tTest Loss: 1.6665\tTrain Accuracy: 0.6935, \tTest Accuracy: 0.4952\n",
      "Epoch 46: \tTrain Loss: 0.8564 \tTest Loss: 1.6732\tTrain Accuracy: 0.6985, \tTest Accuracy: 0.4983\n",
      "Epoch 47: \tTrain Loss: 0.8709 \tTest Loss: 1.6902\tTrain Accuracy: 0.6949, \tTest Accuracy: 0.4857\n",
      "Epoch 48: \tTrain Loss: 0.8597 \tTest Loss: 1.6996\tTrain Accuracy: 0.6970, \tTest Accuracy: 0.4900\n",
      "Epoch 49: \tTrain Loss: 0.8810 \tTest Loss: 1.6755\tTrain Accuracy: 0.6901, \tTest Accuracy: 0.4922\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader for batching\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def one_hot_encoding(targets, num_classes=10, device='cpu'):\n",
    "    # Generate one-hot encoding and move it to the same device as targets\n",
    "    return torch.eye(num_classes, device=device)[targets]\n",
    "\n",
    "# Train function\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        \n",
    "        # One-hot encode the target\n",
    "        target_one_hot = one_hot_encoding(target, num_classes=10, device=device)  # Pass the correct device\n",
    "        \n",
    "        # Calculate loss with MSE\n",
    "        loss = criterion(output, target_one_hot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Track accuracy\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        total += target.size(0)\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = correct / total\n",
    "    return avg_train_loss, train_accuracy\n",
    "\n",
    "# Test function\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            # One-hot encode the target\n",
    "            target_one_hot = one_hot_encoding(target, num_classes=10, device=device)  # Pass the correct device\n",
    "            \n",
    "            # Calculate loss with MSE\n",
    "            test_loss += criterion(output, target_one_hot).item()\n",
    "            \n",
    "            # Track accuracy\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    return avg_test_loss, test_accuracy\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    avg_train_loss, train_accuracy = train(model, device, train_loader, optimizer, epoch)\n",
    "    avg_test_loss, test_accuracy = test(model, device, test_loader)\n",
    "    \n",
    "    train_losses.append(avg_train_loss)\n",
    "    test_losses.append(avg_test_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    print(f'Epoch {epoch}: \\tTrain Loss: {avg_train_loss:.4f} \\tTest Loss: {avg_test_loss:.4f}'\n",
    "          + f'\\tTrain Accuracy: {train_accuracy:.4f}, \\tTest Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# for Normalization & Standardization\n",
    "# 0. 28.4s train_accuracy 0.40, test_acuracy 0.39\n",
    "# 3. The previues attempt has shown progress, but slowly. So enhance learning rate from 0.001 to 0.01 33.4s train_accuracy 0.3545, test_acuracy 0.3269 Epoch 14\n",
    "# 4. reduce learning rate to 0.005 runtime 32.9s train_accuracy 0.3814, test_acuracy 0.3707 Epoch 19\n",
    "# 5. reduce learning rate to 0.003 runtime 32.9s train_accuracy 0.3964, test_acuracy 0.3885 Epoch 19\n",
    "# 6. increase epoch to 30 runtime 49.9s train_accuracy 0.4057, test_acuracy 0.3900 Epoch 29\n",
    "# 7. increase epoch to 50 runtime 49.9s train_accuracy 0.4179, test_acuracy 0.3755 Epoch 47\n",
    "# 16. added normalization function correctly. (used the code in week6) learned that right Normalization is really important. However, has huge overfitting. runtime 1m 54.7s train_accuracy 0.7005, test_acuracy 0.4705 Epoch 46\n",
    "\n",
    "\n",
    "# for Weight Initialization\n",
    "# 9. I have used he function which is used for rellu functions. However, the results hasn't improved. It is due to the architecture isn't big enough runtime 1m 26.2s train_accuracy 0.4914 test_accuracy 0.4392 Epoch 48\n",
    "# 15. deleted he function the train accuracy slightly improved and the test accuracy boosted up. runtime 1m 26.1s train_accuracy 0.5634, test_acuracy 0.4867 Epoch 49\n",
    "\n",
    "# for Architectures\n",
    "# 1. added a hidden 100 Node layer time runtime 32.4s train_accuracy 0.4070, test_acuracy 0.3798 Epoch 17\n",
    "# 2. added a extra hidden 50 Node layerand 20 Node layer time runtime 33.4s train_accuracy 0.4085, test_acuracy 0.3887 Epoch 19\n",
    "# 13. the batch size increased to 256 reduced the runtime and enhanced the accuracy. However, this might give some overfitting problemruntime 1m 15.0s train_accuracy 0.5025, test_acuracy 0.4572 Epoch 49\n",
    "# 14. use vanialla network to use existing functions proven. runtime 1m 24.8s train_accuracy 0.5598, test_acuracy 0.4633 Epoch 48\n",
    "# 16. add dropout the rate is 0.5 for 2 layers. runtime 1m 55.7s train_accuracy 0.3511, test_acuracy 0.3887 Epoch 49\n",
    "# 17. change dropout rate is 0.2 for 2 layers. runtime 1m 55.0s train_accuracy 0.5500, test_acuracy 0.4853 Epoch 49\n",
    "# 18. change dropout rate is 0.1 for 2 layers. runtime 1m 54.2s train_accuracy 0.6350, test_acuracy 0.4979 Epoch 49\n",
    "# 19. delete one dropout layer. train accuracy down, test accuracy up runtime 1m 55.5s train_accuracy 0.6985, test_acuracy 0.4983 Epoch 49\n",
    "\n",
    "# for activation functions\n",
    "# 8. add 2 rellu layers runtime 1m 25.0s train_accuracy 0.5139, test_acuracy 0.4468 Epoch 46\n",
    "\n",
    "\n",
    "# for loss function, Cross Entrophy loss was initially used.\n",
    "# sticked to the original function because Cross entrophy is the most accurate for the test\n",
    "    \n",
    "# for regularization L2 was initially used (1e-4).\n",
    "# 10. updated L2 to 1e-3, which looked successful at first, but later the accuracy stopped at 0.45 ish runtime 1m 26.3s train_accuracy 0.4586, test_acuracy 0.4314 Epoch 47\n",
    "# 11. updated L2 to 5e-4 accuracy increased a bit, stopped at 0.47 runtime 1m 25.3s train_accuracy 0.4717, test_acuracy 0.4224 Epoch 48\n",
    "# 12. tried with 7e-4, but decided to stick to the original L2 value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [40 points] Repeat problem 2 using a CNN and the Imagenette dataset. Run at least 10 training experiments, but you are free to use any techniques you choose, but much of the credit is based on your reasoning: your progression requires rationale for why you're tuning the hyperparmeterse you choose to tune. Required: Experiment with data augmentation. [GPU computing is recommended for this.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-3         [-1, 64, 112, 112]               0\n",
      "            Conv2d-4        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-5        [-1, 128, 112, 112]               0\n",
      "         MaxPool2d-6          [-1, 128, 56, 56]               0\n",
      "            Conv2d-7          [-1, 256, 56, 56]         295,168\n",
      "              ReLU-8          [-1, 256, 56, 56]               0\n",
      "         MaxPool2d-9          [-1, 256, 28, 28]               0\n",
      "          Flatten-10               [-1, 200704]               0\n",
      "           Linear-11                 [-1, 1024]     205,521,920\n",
      "             ReLU-12                 [-1, 1024]               0\n",
      "          Dropout-13                 [-1, 1024]               0\n",
      "           Linear-14                  [-1, 512]         524,800\n",
      "             ReLU-15                  [-1, 512]               0\n",
      "           Linear-16                   [-1, 10]           5,130\n",
      "          Softmax-17                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 206,422,666\n",
      "Trainable params: 206,422,666\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 98.03\n",
      "Params size (MB): 787.44\n",
      "Estimated Total Size (MB): 886.05\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 224, 224]             896\n",
      "       BatchNorm2d-2         [-1, 32, 224, 224]              64\n",
      "         MaxPool2d-3         [-1, 32, 112, 112]               0\n",
      "            Conv2d-4        [-1, 128, 112, 112]          36,992\n",
      "       BatchNorm2d-5        [-1, 128, 112, 112]             256\n",
      "         MaxPool2d-6          [-1, 128, 56, 56]               0\n",
      "            Conv2d-7          [-1, 256, 56, 56]         295,168\n",
      "       BatchNorm2d-8          [-1, 256, 56, 56]             512\n",
      "         MaxPool2d-9          [-1, 256, 28, 28]               0\n",
      "           Conv2d-10          [-1, 512, 28, 28]       1,180,160\n",
      "      BatchNorm2d-11          [-1, 512, 28, 28]           1,024\n",
      "        MaxPool2d-12          [-1, 512, 14, 14]               0\n",
      "           Conv2d-13         [-1, 1024, 14, 14]       4,719,616\n",
      "      BatchNorm2d-14         [-1, 1024, 14, 14]           2,048\n",
      "        MaxPool2d-15           [-1, 1024, 7, 7]               0\n",
      "           Linear-16                 [-1, 1024]      51,381,248\n",
      "          Dropout-17                 [-1, 1024]               0\n",
      "           Linear-18                  [-1, 512]         524,800\n",
      "           Linear-19                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 58,147,914\n",
      "Trainable params: 58,147,914\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 79.26\n",
      "Params size (MB): 221.82\n",
      "Estimated Total Size (MB): 301.65\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "\n",
    "# import v2 for data augmentation\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "# Define data augmentation transforms (if needed)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),         # Resize images to 224x224\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "    \n",
    "])\n",
    "\n",
    "train_transform = v2.Compose([\n",
    "    v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    transforms.ToTensor(),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Download and load the Imagenette dataset\n",
    "train_dataset = datasets.ImageFolder(root='./imagenette2-320/train', transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder(root='./imagenette2-320/val', transform=test_transform)\n",
    "\n",
    "# Create data loaders \n",
    "# do not suffle the data for consistant values\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) \n",
    "\n",
    "# image, label = train_dataset[0]\n",
    "# print(image.shape) torch.Size([3, 224, 224])\n",
    "\n",
    "# import LeNetReg model shown in class first.\n",
    "class model(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(model, self).__init__()\n",
    "        \n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: (64, 160, 160)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(32, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: (128, 80, 80)\n",
    "        \n",
    "        # Third convolutional block\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: (256, 40, 40)\n",
    "        \n",
    "        # Fourth convolutional block\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: (512, 20, 20)\n",
    "        \n",
    "        # Fifth convolutional block\n",
    "        self.conv5 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(1024)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: (1024, 10, 10)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(50176, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First conv block\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Second conv block\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Third conv block\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # Fourth conv block\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        # Fifth conv block\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.pool5(x)\n",
    "        \n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# Initialize model, optimizer, and loss function\n",
    "model = model() \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device = device)\n",
    "\n",
    "summary(model, input_size=(3,224,224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, Loss: 3.0404, Accuracy: 0.1737\n",
      "Test Loss: 2.2739, Accuracy: 0.1480\n",
      "\n",
      "\n",
      "Train Epoch: 1, Loss: 2.1630, Accuracy: 0.1953\n",
      "Test Loss: 2.3230, Accuracy: 0.1060\n",
      "\n",
      "\n",
      "Train Epoch: 2, Loss: 2.1504, Accuracy: 0.2114\n",
      "Test Loss: 2.2601, Accuracy: 0.1468\n",
      "\n",
      "\n",
      "Train Epoch: 3, Loss: 2.1458, Accuracy: 0.2157\n",
      "Test Loss: 2.2681, Accuracy: 0.1763\n",
      "\n",
      "\n",
      "Train Epoch: 4, Loss: 2.1156, Accuracy: 0.2283\n",
      "Test Loss: 2.2517, Accuracy: 0.1651\n",
      "\n",
      "\n",
      "Train Epoch: 5, Loss: 2.0967, Accuracy: 0.2385\n",
      "Test Loss: 2.2407, Accuracy: 0.1623\n",
      "\n",
      "\n",
      "Train Epoch: 6, Loss: 2.0913, Accuracy: 0.2375\n",
      "Test Loss: 2.5557, Accuracy: 0.1022\n",
      "\n",
      "\n",
      "Early stopping after 6 epochs.\n",
      "Test Loss: 2.5557, Accuracy: 0.1022\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Train function\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        \n",
    "        # Calculate loss with CrossEntropyLoss\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track loss and accuracy\n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        total += target.size(0)\n",
    "    \n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    print(f'Train Epoch: {epoch}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# Test function\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "    \n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    accuracy = correct / total\n",
    "    print(f'Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# exp 1. added for non-increasing accuracy \n",
    "best_val_loss = float('inf')\n",
    "best_accuracy = 0\n",
    "patience = 7  # stop training if no improvement for 'patience' epochs\n",
    "loss_patience_counter = 0\n",
    "accuracy_patience_counter = 0\n",
    "    \n",
    "\n",
    "# Training and validation loop\n",
    "num_epochs = 20\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train(model, device, train_loader, optimizer, epoch)\n",
    "    val_loss, val_accuracy = test(model, device, test_loader)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print('\\n')\n",
    "    \n",
    "    # exp 1. added for non-increasing accuracy \n",
    "    if val_loss < best_val_loss :\n",
    "        best_val_loss = val_loss\n",
    "        loss_patience_counter = 0  # Reset the patience counter\n",
    "    else:\n",
    "        loss_patience_counter += 1\n",
    "\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        accuracy_patience_counter = 0  # Reset the patience counter on improvement\n",
    "    else:\n",
    "        accuracy_patience_counter += 1\n",
    "\n",
    "        \n",
    "    if (loss_patience_counter >= patience) or (accuracy_patience_counter >= patience):\n",
    "        print(f\"Early stopping after {epoch} epochs.\")\n",
    "        break\n",
    "\n",
    "# Test the model on the test set\n",
    "test_loss, test_accuracy = test(model, device, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Experiment Track\n",
    "| Exp num | Description(change) | Note | Epoch | Train_accuracy | Test_accuracy | rumtime |\n",
    "| ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | \n",
    "| 0. | initial run with data augmentation(flip) <br> cross entrophy loss <br> lr = 0.001 |training accuracy went up, but test accuracy went down. overfitting is the problem| 20 | 0.5703 | 0.1516 | 34m 3.4s|\n",
    "| 1. | 1. add early stopping(stop when validation is not increasing) to prevent time loss <br> 2. L2 regularization to reduce overfitting weight_decay=1e-4| overfitting is still a problem. <br> went to 15 epoch, and the highest accuracy was epoch 10. <br> will include dropout and increase the number of layers | 10 | 0.5760 | 0.1299 | 27m 46.0s |\n",
    "| 2. | added extra layers and added dropout| Validation accuracy increased slightly. <br> But more time spent| 9 | 0.4541 | 0.2293 | 60m 49.3s |\n",
    "| 3. | add full connected layers, and increase dropout rate.<br> [0.2,0.2,0.2,0.4] -> [0.3, 0.3, 0.3, 0.5] <br> fc(90,10) -> fc(90,50,10)|No Improvement| 6 | 0.3974 | 0.1768 | 42m 55.8s |\n",
    "| 4. | roll back dropout rate |No Improvement| 5 | 0.3924 | 0.1332 | 36m 52.1s |\n",
    "| 5. | add one more fc layer <br> increase patience number to 7 to see more results <br> fc(90,50,10) -> fc(90,128,64,10)|Accuracy went slightly up, but no improvement| 10 | 0.5907 | 0.1847| 69m 20.0s |\n",
    "| 6. | couldn't figure out how to improve the CNN, so implememted the same architecture in Class | the accuracy fell down. <br>Reverse to previous CNN| 6 | 0.0980| 0.1068| 117m 31.3s |\n",
    "| 7. | correctly implemeted dropout. | the accuracy fell down. <br>Reverse to previous CNN| 6 | 0.0980| 0.1068| 117m 31.3s |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
