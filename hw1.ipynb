{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [10 points] Write a version of gradient descent that chooses n random starting points and outputs the parameters resulting in minimum training loss across all the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLeastSquaresGradient\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m         \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# fit the model to the data\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, w0, alpha, h, tolerance, max_iterations):\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# save the training data\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "class LeastSquaresGradient:\n",
    "        \n",
    "    # fit the model to the data\n",
    "    def fit(self, X, y, w0, alpha, h, tolerance, max_iterations):\n",
    "\n",
    "        # save the training data\n",
    "        X = np.hstack((np.ones([X.shape[0], 1]), X))\n",
    "        \n",
    "        # find the w values that minimize the sum of squared errors via gradient descent\n",
    "        L = lambda w: ((X @ w).T - y.T) @ (X @ w - y)\n",
    "        self.w = self.gradientDescent(L, w0, alpha, h, tolerance, max_iterations)\n",
    "                \n",
    "    # predict the output from testing data\n",
    "    def predict(self, X):        \n",
    "        # append a column of ones at the beginning of X\n",
    "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "        \n",
    "        # return the predicted y's\n",
    "        return X @ self.w\n",
    "\n",
    "    # run gradient descent to minimize the loss function\n",
    "    def gradientDescent(self, f, x0, alpha, h, tolerance, max_iterations):\n",
    "        # set x equal to the initial guess\n",
    "        x = x0\n",
    "\n",
    "        # take up to maxIterations number of steps\n",
    "        for counter in range(max_iterations):\n",
    "            # update the gradient\n",
    "            gradient = self.computeGradient(f, x, h)\n",
    "\n",
    "            # stop if the norm of the gradient is near 0\n",
    "            if np.linalg.norm(gradient) < tolerance:\n",
    "                print('Gradient descent took', counter, 'iterations to converge')\n",
    "                print('The norm of the gradient is', np.linalg.norm(gradient))\n",
    "                \n",
    "                # return the approximate critical value x\n",
    "                return x\n",
    "\n",
    "            # if we do not converge, print a message\n",
    "            elif counter == max_iterations - 1:\n",
    "                print(\"Gradient descent failed\")\n",
    "                print('The gradient is', gradient)\n",
    "                \n",
    "                # return x, sometimes it is still pretty good\n",
    "                return x\n",
    "\n",
    "            # take a step in the opposite direction as the gradient\n",
    "            x -= alpha*gradient\n",
    "            \n",
    "    # estimate the gradient\n",
    "    def computeGradient(self, f, x, h):\n",
    "        n = len(x)\n",
    "        gradient = np.zeros(n)\n",
    "        \n",
    "        # compute f at current point\n",
    "        fx = f(x)\n",
    "\n",
    "        # find each component of the gradient\n",
    "        for counter in range(n):\n",
    "            xUp = x.copy()\n",
    "            xUp[counter] += h\n",
    "            gradient[counter] = (f(xUp) - fx)/h\n",
    "\n",
    "        # return the gradient\n",
    "        return gradient\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the model...\n",
      "\n",
      "Gradient descent took 18109 iterations to converge\n",
      "The norm of the gradient is 0.009999497957967543\n",
      "\n",
      "The predicted y values are [1.89 2.4  2.91 3.41 2.4 ]\n",
      "The real y values are [1 2 3 3 4]\n",
      "The w values are [-1.1588022   0.50801263]\n"
     ]
    }
   ],
   "source": [
    "# check model\n",
    "\n",
    "X = np.array([[6], [7], [8], [9], [7]])\n",
    "y = np.array([1, 2, 3, 3, 4])\n",
    "\n",
    "# instantiate an least squares object, fit to data, predict data\n",
    "model = LeastSquaresGradient()\n",
    "\n",
    "print('Fitting the model...\\n')\n",
    "model.fit(X, y, w0 = np.random.rand(X.shape[1]+1), alpha=0.001, h=0.001, tolerance=0.01, max_iterations=100000)\n",
    "\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# print the predictions\n",
    "print('\\nThe predicted y values are', np.round(predictions, 2))\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values are', y)\n",
    "\n",
    "# print the w values\n",
    "parameters = model.w\n",
    "print('The w values are', parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [5 points] Write a version of gradient descent that uses momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# estimate the gradient\n",
    "def computeGradientWithMomentum(f, x, h, pastGradient):\n",
    "    n = len(x)\n",
    "    gradient = np.zeros(n)\n",
    "    pastGradient = np.array(pastGradient)\n",
    "    for counter in range(n):\n",
    "        xUp = x.copy()\n",
    "        xUp[counter] += h\n",
    "        gradient[counter] = (f(xUp) - f(x))/h - pastGradient[counter]*0.02\n",
    "            \n",
    "    return gradient, pastGradient\n",
    "\n",
    "    # run gradient descent and output the coordinates of the estimated critical point\n",
    "def gradientDescent(f, x0, alpha, h, tolerance, maxIterations):\n",
    "    # set x equal to the initial guess\n",
    "    x = x0\n",
    "    # make a pastGradient parameter to store previous gradients\n",
    "    pastGradient = np.zeros(x)\n",
    "    # take up to maxIterations number of steps\n",
    "    for counter in range(maxIterations):\n",
    "        # update the gradient and store the previous gradient\n",
    "        gradient,pastGradient = computeGradientWithMomentum(f, x, h, pastGradient)\n",
    "        pastGradient = gradient\n",
    "        # stop if the norm of the gradient is near 0 (success)\n",
    "        if np.linalg.norm(gradient) < tolerance:\n",
    "            print('Gradient descent took', counter, 'iterations to converge')\n",
    "            print('The norm of the gradient is', np.linalg.norm(gradient))\n",
    "            \n",
    "            # return the approximate critical point x\n",
    "            return x\n",
    "            \n",
    "        # print a message if we do not converge (failure)\n",
    "        elif counter == maxIterations-1:\n",
    "            print(\"Gradient descent failed\")\n",
    "            print('The gradient is', gradient)\n",
    "                \n",
    "            # return x, sometimes it is still pretty good\n",
    "            return x\n",
    "            \n",
    "        # take a step in the opposite direction as the gradient\n",
    "        x -= alpha*gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent took 18 iterations to converge\n",
      "The norm of the gradient is 9.976123420763023e-05\n",
      "[4.46228432] [-0.96888652]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b8/xxt8cq0j5s50gztcml150m700000gn/T/ipykernel_10571/2941004565.py:9: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  gradient[counter] = (f(xUp) - f(x))/h - pastGradient[counter]*0.02\n"
     ]
    }
   ],
   "source": [
    "# test sin(x)\n",
    "f = lambda x: np.sin(x)\n",
    "\n",
    "x = gradientDescent(f,[2],0.5,0.5,0.0001,10000)\n",
    "print(x, f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent took 12 iterations to converge\n",
      "The norm of the gradient is 2.890914172880595e-07\n",
      "[-0.19999984] 0.03999993769327916\n"
     ]
    }
   ],
   "source": [
    "# test x^2\n",
    "f = lambda x : x[0]**2\n",
    "\n",
    "x = gradientDescent(f,[2],0.4,0.4,0.000001,10000)\n",
    "print(x, f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [10 points] Implement a linear regression class in the style of scikit-learn regressors (i.e. as a Python class with fit and predict functions), with capabilities to use n random starting points and momentum (default to not using these features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "class question3:\n",
    "    def __init__(self, use_momentum, num_starting_points, momentum_factor, learning_rate):\n",
    "        self.num_starting_points = num_starting_points\n",
    "        self.use_momentum = use_momentum\n",
    "        self.momentum_factor = momentum_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        self.X = np.array(X)\n",
    "        self.y = np.array(y)\n",
    "        n_classes = len(np.unique(y))\n",
    "\n",
    "        # m = n_features\n",
    "        n_samples,n_features = X.shape\n",
    "        # set up random factors in weight w and bias b\n",
    "        self.W = np.random.randn(n_features, n_classes, self.num_starting_points) # must be 3d due to multiple starting points\n",
    "        self.b = np.zeros((n_classes, self.num_starting_points))\n",
    "\n",
    "        # self.GradientDescent(self.X,self.y,self.w,self.b)\n",
    "\n",
    "        if self.use_momentum:\n",
    "            self.velocity_W = np.zeros((n_features, n_classes, self.num_starting_points))\n",
    "            self.velocity_b = np.zeros((n_classes, self.num_starting_points))\n",
    "        \n",
    "        best_cost = float('inf') # intially save the largest number to save lower costs\n",
    "        best_W = None\n",
    "        best_b = None\n",
    "\n",
    "        # Gradient descent for each starting points\n",
    "        for start_point in range(self.num_starting_points):\n",
    "            W = self.W[:, :, start_point]\n",
    "            b = self.b[:, start_point]\n",
    "\n",
    "            if self.use_momentum:\n",
    "                velocity_W = self.velocity_W[:, :, start_point]\n",
    "                velocity_b = self.velocity_b[:, start_point]\n",
    "\n",
    "            for iteration in range(1000):\n",
    "                dW, db = self.compute_gradient(X, y, W, b)\n",
    "\n",
    "                if self.use_momentum:\n",
    "                    # Update momentum terms\n",
    "                    velocity_W = self.momentum_factor * velocity_W + (1 - self.momentum_factor) * dW\n",
    "                    velocity_b = self.momentum_factor * velocity_b + (1 - self.momentum_factor) * db\n",
    "                    \n",
    "                    # Update weights and biases with momentum\n",
    "                    W -= self.learning_rate * velocity_W\n",
    "                    b -= self.learning_rate * velocity_b\n",
    "                else:\n",
    "                    # Update weights and biases without momentum\n",
    "                    W -= self.learning_rate * dW\n",
    "                    b -= self.learning_rate * db\n",
    "                \n",
    "                # Calculate cost to monitor the training process\n",
    "                if iteration % 100 == 0:\n",
    "                    cost = self.compute_cost(X, y, W, b)\n",
    "                    print(f\"Starting Point {start_point}, Iteration {iteration}, Cost: {cost}\")\n",
    "\n",
    "                # Keep track of the best weights and biases based on cost\n",
    "                if cost < best_cost:\n",
    "                    best_cost = cost\n",
    "                    best_W = W\n",
    "                    best_b = b\n",
    "                    \n",
    "            # Save the best weights and biases for current starting point\n",
    "            self.W[:, :, start_point] = W\n",
    "            self.b[:, start_point] = b\n",
    "\n",
    "        # Set the best weights and bias\n",
    "        self.W = best_W\n",
    "        self.b = best_b\n",
    "        \n",
    "    \n",
    "    def compute_cost(self,X,y,w,b):\n",
    "        n = len(y)\n",
    "        y_predict = np.dot(X, w) + b  # prediction\n",
    "        cost = (1 / (2 * n)) * np.sum((y_predict - y) ** 2)\n",
    "        return cost\n",
    "    \n",
    "    def GradientDescent(self,X,y,w,b):\n",
    "        # initialize w and b\n",
    "        self.velocity_w = np.zeros_like(self.w)\n",
    "        self.velocity_b = 0\n",
    "\n",
    "        for i in range(1000):\n",
    "            dw, db = self.computeGradient(X, y, self.w, self.b)\n",
    "\n",
    "            # set learning rate to 0.04 , momentum rate to 0.01\n",
    "            self.w = self.w - dw * 0.04 - self.velocity_w * 0.01\n",
    "            self.b = self.b - db * 0.04 - self.velocity_b * 0.01\n",
    "            \n",
    "            self.velocity_b = copy.deepcopy(self.b)\n",
    "            self.velocity_w = copy.deepcopy(self.w)\n",
    "\n",
    "            cost = self.computeCost(X, y, self.w, self.b)\n",
    "            if i % 100 == 0:  # \n",
    "                print(f\"Iteration {i}, Cost: {cost}\")\n",
    "            \n",
    "            if np.abs(dw) < 0.1:\n",
    "                print(self.w)\n",
    "                print(f\"Cost is: {cost}\")\n",
    "                print(i , \" iterations\")\n",
    "                break\n",
    "        return\n",
    "    \n",
    "    def compute_gradient(self,X,y,W,b):\n",
    "        n_samples = X.shape[0]\n",
    "        dw = 0\n",
    "        db = 0\n",
    "        z = np.dot(X, W) + b  # Shape: (n_samples, n_classes)\n",
    "        \n",
    "        # Compute gradients\n",
    "        error = z - y\n",
    "        dW = np.dot(X.T, error) / n_samples\n",
    "        db = np.sum(error, axis=0) / n_samples\n",
    "\n",
    "        return dw, db\n",
    "    \n",
    "    def predict(self, x):\n",
    "        predict_y = np.dot(x,self.W) + self.b\n",
    "        return predict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Point 0, Iteration 0, Cost: 108.86924305692764\n",
      "Starting Point 0, Iteration 100, Cost: 4.125646765278932\n",
      "Starting Point 0, Iteration 200, Cost: 2.2978774732733385\n",
      "Starting Point 0, Iteration 300, Cost: 2.2659955937812186\n",
      "Starting Point 0, Iteration 400, Cost: 2.2654394764176122\n",
      "Starting Point 0, Iteration 500, Cost: 2.2654297760320956\n",
      "Starting Point 0, Iteration 600, Cost: 2.2654296068277415\n",
      "Starting Point 0, Iteration 700, Cost: 2.2654296038763007\n",
      "Starting Point 0, Iteration 800, Cost: 2.2654296038248187\n",
      "Starting Point 0, Iteration 900, Cost: 2.26542960382392\n",
      "Starting Point 1, Iteration 0, Cost: 147.85881100595248\n",
      "Starting Point 1, Iteration 100, Cost: 3.689014357780245\n",
      "Starting Point 1, Iteration 200, Cost: 1.173260302377151\n",
      "Starting Point 1, Iteration 300, Cost: 1.1293778671174108\n",
      "Starting Point 1, Iteration 400, Cost: 1.1286124234047434\n",
      "Starting Point 1, Iteration 500, Cost: 1.1285990717282972\n",
      "Starting Point 1, Iteration 600, Cost: 1.1285988388342765\n",
      "Starting Point 1, Iteration 700, Cost: 1.1285988347718934\n",
      "Starting Point 1, Iteration 800, Cost: 1.128598834701033\n",
      "Starting Point 1, Iteration 900, Cost: 1.1285988346997968\n",
      "Starting Point 2, Iteration 0, Cost: 160.89055822471158\n",
      "Starting Point 2, Iteration 100, Cost: 5.665449482524652\n",
      "Starting Point 2, Iteration 200, Cost: 2.956780913257493\n",
      "Starting Point 2, Iteration 300, Cost: 2.9095334596060995\n",
      "Starting Point 2, Iteration 400, Cost: 2.9087093196940152\n",
      "Starting Point 2, Iteration 500, Cost: 2.90869494417656\n",
      "Starting Point 2, Iteration 600, Cost: 2.908694693423625\n",
      "Starting Point 2, Iteration 700, Cost: 2.9086946890497276\n",
      "Starting Point 2, Iteration 800, Cost: 2.908694688973433\n",
      "Starting Point 2, Iteration 900, Cost: 2.908694688972103\n",
      "Starting Point 3, Iteration 0, Cost: 131.44916640320525\n",
      "Starting Point 3, Iteration 100, Cost: 4.380239425371991\n",
      "Starting Point 3, Iteration 200, Cost: 2.162894490494088\n",
      "Starting Point 3, Iteration 300, Cost: 2.124217221806269\n",
      "Starting Point 3, Iteration 400, Cost: 2.1235425720981342\n",
      "Starting Point 3, Iteration 500, Cost: 2.1235308041464642\n",
      "Starting Point 3, Iteration 600, Cost: 2.1235305988774402\n",
      "Starting Point 3, Iteration 700, Cost: 2.123530595296922\n",
      "Starting Point 3, Iteration 800, Cost: 2.1235305952344663\n",
      "Starting Point 3, Iteration 900, Cost: 2.123530595233378\n",
      "Starting Point 4, Iteration 0, Cost: 113.56023129872179\n",
      "Starting Point 4, Iteration 100, Cost: 9.811728957356706\n",
      "Starting Point 4, Iteration 200, Cost: 8.001323993380536\n",
      "Starting Point 4, Iteration 300, Cost: 7.969745000809713\n",
      "Starting Point 4, Iteration 400, Cost: 7.969194166719202\n",
      "Starting Point 4, Iteration 500, Cost: 7.969184558490103\n",
      "Starting Point 4, Iteration 600, Cost: 7.96918439089324\n",
      "Starting Point 4, Iteration 700, Cost: 7.969184387969837\n",
      "Starting Point 4, Iteration 800, Cost: 7.969184387918844\n",
      "Starting Point 4, Iteration 900, Cost: 7.969184387917955\n",
      "Predictions: [[2.51410484 2.61808368 4.84475315 5.48234741 9.44302305]\n",
      " [2.68547312 2.15744491 4.45967087 4.64312989 9.25736408]\n",
      " [2.8568414  1.69680614 4.07458859 3.80391237 9.0717051 ]]\n"
     ]
    }
   ],
   "source": [
    "# Example input data\n",
    "X = np.array([[1], [2], [3], [4], [5]])  # Input feature values\n",
    "y = np.array([2, 4, 6, 8, 10])  # Target output values (simple linear relation: y = 2 * X)\n",
    "\n",
    "# Initialize and fit the model\n",
    "model = question3(num_starting_points = 5, momentum_factor = 0.09, use_momentum = True, learning_rate=0.02)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Test the model's prediction\n",
    "X_test = np.array([[6], [7], [8]])  # Test inputs\n",
    "predictions = model.predict(X_test)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. [10 points] Train and tune a linear regression model to predict death rates from cancer in US counties using the OLS Regression Challenge. During tuning, perform hyperparameter searches with n starting points, momentum, both, and neither. Use data from 30 random U.S. states for training, 10 random U.S. states for validation, and 10 random U.S. states for testing. Compare the results in terms of test accuracy, fit runtime, and predict runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[245], line 48\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Display shapes to verify\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# print(f\"Training data shape: X_train={X_train.shape}, y_train={y_train.shape}\")\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# print(f\"Validation data shape: X_val={X_val.shape}, y_val={y_val.shape}\")\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# print(f\"Test data shape: X_test={X_test.shape}, y_test={y_test.shape}\")\u001b[39;00m\n\u001b[1;32m     46\u001b[0m model2 \u001b[38;5;241m=\u001b[39m question3(num_starting_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, momentum_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.09\u001b[39m, use_momentum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.02\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m model2\u001b[38;5;241m.\u001b[39mfit(X_train,y_train)\n\u001b[1;32m     49\u001b[0m model2\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "Cell \u001b[0;32mIn[243], line 40\u001b[0m, in \u001b[0;36mquestion3.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     37\u001b[0m     velocity_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvelocity_b[:, start_point]\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m---> 40\u001b[0m     dW, db \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_gradient(X, y, W, b)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_momentum:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;66;03m# Update momentum terms\u001b[39;00m\n\u001b[1;32m     44\u001b[0m         velocity_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum_factor \u001b[38;5;241m*\u001b[39m velocity_W \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum_factor) \u001b[38;5;241m*\u001b[39m dW\n",
      "Cell \u001b[0;32mIn[243], line 111\u001b[0m, in \u001b[0;36mquestion3.compute_gradient\u001b[0;34m(self, X, y, W, b)\u001b[0m\n\u001b[1;32m    109\u001b[0m dw \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    110\u001b[0m db \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 111\u001b[0m z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X, W) \u001b[38;5;241m+\u001b[39m b  \u001b[38;5;66;03m# Shape: (n_samples, n_classes)\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[1;32m    114\u001b[0m error \u001b[38;5;241m=\u001b[39m z \u001b[38;5;241m-\u001b[39m y\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read CSV file\n",
    "data = pd.read_csv('cancer_reg.csv', encoding='latin-1') # Use latin-1 to avoid unicode errors\n",
    "\n",
    "# Extract target and feature columns\n",
    "y = data[\"TARGET_deathRate\"]\n",
    "X = data.drop(columns=[\"TARGET_deathRate\"])\n",
    "\n",
    "# Extract states from the 'Geography' column\n",
    "states = data['Geography'].str.split(\", \").str[1].unique() # Get unique state names\n",
    "\n",
    "# Add the cleaned 'Geography' column to X\n",
    "X['Geography'] = data['Geography'].str.split(\", \").str[1]\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Choose 30 | 10 | 10 states for training, validation, and testing\n",
    "train_states = np.random.choice(states, 30, replace=False)\n",
    "remaining_states = [state for state in states if state not in train_states]\n",
    "val_states = np.random.choice(remaining_states, 10, replace=False)\n",
    "test_states = [state for state in remaining_states if state not in val_states]\n",
    "\n",
    "# Split data based on these states\n",
    "train_data = data[data['Geography'].str.split(\", \").str[1].isin(train_states)]\n",
    "val_data = data[data['Geography'].str.split(\", \").str[1].isin(val_states)]\n",
    "test_data = data[data['Geography'].str.split(\", \").str[1].isin(test_states)]\n",
    "\n",
    "# Prepare the feature and target sets\n",
    "X_train = train_data.drop(columns=[\"TARGET_deathRate\", \"Geography\"])\n",
    "y_train = train_data[\"TARGET_deathRate\"]\n",
    "\n",
    "X_val = val_data.drop(columns=[\"TARGET_deathRate\", \"Geography\"])\n",
    "y_val = val_data[\"TARGET_deathRate\"]\n",
    "\n",
    "X_test = test_data.drop(columns=[\"TARGET_deathRate\", \"Geography\"])\n",
    "y_test = test_data[\"TARGET_deathRate\"]\n",
    "\n",
    "# Display shapes to verify\n",
    "# print(f\"Training data shape: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "# print(f\"Validation data shape: X_val={X_val.shape}, y_val={y_val.shape}\")\n",
    "# print(f\"Test data shape: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "model2 = question3(num_starting_points = 5, momentum_factor = 0.09, use_momentum = True, learning_rate=0.02)\n",
    "\n",
    "model2.fit(X_train,y_train)\n",
    "model2.predict(X_val)\n",
    "\n",
    "## couldn't solve due to lack of knowledge in pandas. I will work on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. [10 points] Derive an explicit formula for the gradient of the sum of squared errors loss for a logistic classifier with $k$\n",
    " classes for input data of dimension $d+1$ and one-hot labels. [Hint. Use the softmax function instead of sigmoid.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax_regression:\n",
    "    def __init__(self, learning_rate,max_iter):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "    def fit(self,X,y):\n",
    "        # m = number of samples\n",
    "        # n = number of features\n",
    "        m,n = X.shape \n",
    "        # get number of unique inputs\n",
    "        n_classes = len(np.unique(y))\n",
    "\n",
    "        # initialize weight + bias\n",
    "        self.W = np.random.randn(n, n_classes)\n",
    "        self.b = np.zeros((1, n_classes))\n",
    "\n",
    "        # convert y to one-hot labels\n",
    "        y_one_hot = np.eye(n_classes)[y]\n",
    "\n",
    "         # Gradient Descent Loop\n",
    "        for i in range(self.max_iter):\n",
    "            dW, db = self.compute_gradient(X, y_one_hot, self.W, self.b)\n",
    "            \n",
    "            # Update weights and bias\n",
    "            self.W -= self.learning_rate * dW\n",
    "            self.b -= self.learning_rate * db\n",
    "            \n",
    "            # Optionally print the cost every 100 iterations\n",
    "            if i % 100 == 0:\n",
    "                cost = self.compute_cost(X, y_one_hot, self.W, self.b)\n",
    "                print(f\"Iteration {i}, Cost: {cost}\")\n",
    "\n",
    "    def compute_gradient(self, X, y_one_hot, W, b):\n",
    "        n_samples = X.shape[0]\n",
    "        z = np.dot(X, W) + b \n",
    "\n",
    "        # make prediction with softmax function\n",
    "        # y_pred.shape = m,n\n",
    "        y_pred = self.softmax(z)\n",
    "\n",
    "        error = y_pred - y_one_hot\n",
    "        dW = np.dot(X.T, error) / n_samples\n",
    "        db = np.sum(error, axis=0, keepdims=True) / n_samples\n",
    "\n",
    "        return dW, db\n",
    "        \n",
    "    def softmax(self, z):\n",
    "\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Subtract max for numerical stability\n",
    "\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)  # Compute probabilities\n",
    "\n",
    "    def compute_cost(self, X, y_one_hot, W, b):\n",
    "        n_samples = X.shape[0]\n",
    "        z = np.dot(X, W) + b  # (shape: [n_samples, n_classes])\n",
    "        y_pred = self.softmax(z)  # Apply softmax to get class probabilities\n",
    "            \n",
    "        # Sum of Squared Errors loss / num of samples\n",
    "        cost = (1 / 2) * np.sum((y_one_hot - y_pred) ** 2) / n_samples\n",
    "        return cost\n",
    "    def predict(self, X):\n",
    "\n",
    "        z = np.dot(X, self.W) + self.b  # Linear combination\n",
    "        y_pred = self.softmax(z)  # Apply softmax to get probabilities\n",
    "        return np.argmax(y_pred, axis=1)  # Return the index of the highest probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Cost: 0.36318778042735717\n",
      "Iteration 100, Cost: 0.34358702306708594\n",
      "Iteration 200, Cost: 0.334286064837539\n",
      "Iteration 300, Cost: 0.3285529132497018\n",
      "Iteration 400, Cost: 0.32505085543734447\n",
      "Iteration 500, Cost: 0.32289269592887165\n",
      "Iteration 600, Cost: 0.3215381261306709\n",
      "Iteration 700, Cost: 0.32066884563250925\n",
      "Iteration 800, Cost: 0.3200982424761425\n",
      "Iteration 900, Cost: 0.3197157935470677\n",
      "Predicted class labels: [0 1 0 1 1 0 0 0 0 0 1 2 1 1 0 1 1 2 2 1 0 0 0 2 0 2 1 1 0 2 0 0 1 0 0 0 1\n",
      " 0 2 0 2 0 2 1 0 0 0 0 1 1 1 2 2 1 1 1 0 0 1 0 0 1 1 0 2 0 2 0 1 1 0 0 0 2\n",
      " 0 0 2 0 0 0 2 1 2 1 1 2 1 1 2 1 2 1 2 2 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 3)  # 100 samples, 3 features (dimension d + 1)\n",
    "y = np.random.randint(0, 3, 100)  # 3 classes (k = 3)\n",
    "\n",
    "# Create and train the model\n",
    "model = softmax_regression(learning_rate=0.1, max_iter=1000)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict on training data\n",
    "predictions = model.predict(X)\n",
    "print(\"Predicted class labels:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. [5 points] Repeat Problem 5 with the cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for question 6, the code is similar to question 5 and I have just changed the compute_cost function.\n",
    "\n",
    "class cross_entry_regression:\n",
    "    def __init__(self, learning_rate,max_iter):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "    def fit(self,X,y):\n",
    "        # m = number of samples\n",
    "        # n = number of features\n",
    "        m,n = X.shape \n",
    "        # get number of unique inputs\n",
    "        n_classes = len(np.unique(y))\n",
    "\n",
    "        # initialize weight + bias\n",
    "        self.W = np.random.randn(n, n_classes)\n",
    "        self.b = np.zeros((1, n_classes))\n",
    "\n",
    "        # convert y to one-hot labels\n",
    "        y_one_hot = np.eye(n_classes)[y]\n",
    "\n",
    "         # Gradient Descent Loop\n",
    "        for i in range(self.max_iter):\n",
    "            dW, db = self.compute_gradient(X, y_one_hot, self.W, self.b)\n",
    "            \n",
    "            # Update weights and bias\n",
    "            self.W -= self.learning_rate * dW\n",
    "            self.b -= self.learning_rate * db\n",
    "            \n",
    "            # Optionally print the cost every 100 iterations\n",
    "            if i % 100 == 0:\n",
    "                cost = self.compute_cost_cross_entry(X, y_one_hot, self.W, self.b).T\n",
    "                print(f\"Iteration {i}, Cost: {cost}\")\n",
    "\n",
    "    def compute_gradient(self, X, y_one_hot, W, b):\n",
    "        n_samples = X.shape[0]\n",
    "        z = np.dot(X, W) + b \n",
    "\n",
    "        # make prediction with softmax function\n",
    "        # y_pred.shape = m,n\n",
    "        y_pred = self.softmax(z)\n",
    "\n",
    "        error = y_pred - y_one_hot\n",
    "        dW = np.dot(X.T, error) / n_samples\n",
    "        db = np.sum(error, axis=0, keepdims=True) / n_samples\n",
    "\n",
    "        return dW, db\n",
    "        \n",
    "    def softmax(self, z):\n",
    "\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Subtract max for numerical stability\n",
    "\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)  # Compute probabilities\n",
    "\n",
    "    def compute_cost_cross_entry(self, X, y_one_hot, W, b):\n",
    "        n_samples = X.shape[0]\n",
    "        z = np.dot(X, W) + b  # (shape: [n_samples, n_classes])\n",
    "        y_pred = self.softmax(z)  # Apply softmax to get class probabilities\n",
    "            \n",
    "        # Sum of Squared Errors loss / num of samples\n",
    "        cost = np.sum(np.dot(y_one_hot.T,y_pred)) / n_samples\n",
    "        return cost\n",
    "    \n",
    "    def predict(self, X):\n",
    "\n",
    "        z = np.dot(X, self.W) + self.b  # Linear combination\n",
    "        y_pred = self.softmax(z)  # Apply softmax to get probabilities\n",
    "        return np.argmax(y_pred, axis=1)  # Return the index of the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Cost: 1.0\n",
      "Iteration 100, Cost: 1.0\n",
      "Iteration 200, Cost: 0.9999999999999999\n",
      "Iteration 300, Cost: 1.0\n",
      "Iteration 400, Cost: 0.9999999999999999\n",
      "Iteration 500, Cost: 1.0\n",
      "Iteration 600, Cost: 1.0\n",
      "Iteration 700, Cost: 1.0\n",
      "Iteration 800, Cost: 1.0\n",
      "Iteration 900, Cost: 1.0\n",
      "Predicted class labels: [0 1 0 1 1 0 0 0 0 0 1 2 1 1 0 1 1 2 2 1 0 0 0 2 0 2 1 1 0 2 0 0 1 0 0 0 1\n",
      " 0 2 0 2 0 2 1 0 0 0 0 1 1 1 2 2 1 1 1 0 0 1 0 0 1 1 0 2 0 2 0 1 1 0 0 0 2\n",
      " 0 0 2 0 0 0 2 1 2 1 1 2 1 1 2 1 2 1 2 2 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 3)  # 100 samples, 3 features (dimension d + 1)\n",
    "y = np.random.randint(0, 3, 100)  # 3 classes (k = 3)\n",
    "\n",
    "# Create and train the model\n",
    "model = cross_entry_regression(learning_rate=0.1, max_iter=1000)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict on training data\n",
    "predictions = model.predict(X)\n",
    "print(\"Predicted class labels:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. [10 points] Write a multi-class logistic classifier (for $k$ classes) in the style of scikit-learn classifiers (i.e. as a Python class with fit and predict functions), optimized by gradient descent with options to use multiple starting points and momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiClassLogisticClassifier:\n",
    "    def __init__(self, learning_rate = 0.01, max_iter = 1000, num_starting_points = 1, use_momentum = False, momentum_factor = 0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.num_starting_points = num_starting_points\n",
    "        self.use_momentum = use_momentum\n",
    "        self.momentum_factor = momentum_factor\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y)) # get unique input of y's\n",
    "        \n",
    "        # Initialize weights and biases for multiple starting points\n",
    "        self.W = np.random.randn(n_features, n_classes, self.num_starting_points) # must be 3d due to multiple starting points\n",
    "        self.b = np.zeros((n_classes, self.num_starting_points))\n",
    "\n",
    "        # Initialize momentum for gradient descent\n",
    "        if self.use_momentum:\n",
    "            self.velocity_W = np.zeros((n_features, n_classes, self.num_starting_points))\n",
    "            self.velocity_b = np.zeros((n_classes, self.num_starting_points))\n",
    "        \n",
    "        # Convert y to one-hot encoding\n",
    "        y_one_hot = np.eye(n_classes)[y]\n",
    "        \n",
    "        # save the best weight and bias\n",
    "        best_cost = float('inf') # intially save the largest number to save lower costs\n",
    "        best_W = None\n",
    "        best_b = None\n",
    "\n",
    "        # Gradient Descent for each starting points\n",
    "        for start_point in range(self.num_starting_points):\n",
    "            W = self.W[:, :, start_point]\n",
    "            b = self.b[:, start_point]\n",
    "\n",
    "            if self.use_momentum:\n",
    "                velocity_W = self.velocity_W[:, :, start_point]\n",
    "                velocity_b = self.velocity_b[:, start_point]\n",
    "\n",
    "            for iteration in range(self.max_iter):\n",
    "                # Compute gradient\n",
    "                dW, db = self.compute_gradient(X, y_one_hot, W, b)\n",
    "                \n",
    "                if self.use_momentum:\n",
    "                    # Update momentum terms\n",
    "                    velocity_W = self.momentum_factor * velocity_W + (1 - self.momentum_factor) * dW\n",
    "                    velocity_b = self.momentum_factor * velocity_b + (1 - self.momentum_factor) * db\n",
    "                    \n",
    "                    # Update weights and biases with momentum\n",
    "                    W -= self.learning_rate * velocity_W\n",
    "                    b -= self.learning_rate * velocity_b\n",
    "                else:\n",
    "                    # Update weights and biases without momentum\n",
    "                    W -= self.learning_rate * dW\n",
    "                    b -= self.learning_rate * db\n",
    "\n",
    "                # Calculate cost to monitor the training process\n",
    "                if iteration % 100 == 0:\n",
    "                    cost = self.compute_cost(X, y_one_hot, W, b)\n",
    "                    print(f\"Starting Point {start_point}, Iteration {iteration}, Cost: {cost}\")\n",
    "\n",
    "                # Keep track of the best weights and biases based on cost\n",
    "                if cost < best_cost:\n",
    "                    best_cost = cost\n",
    "                    best_W = W\n",
    "                    best_b = b\n",
    "\n",
    "            # Save the best weights and biases for current starting point\n",
    "            self.W[:, :, start_point] = W\n",
    "            self.b[:, start_point] = b\n",
    "        \n",
    "        # Set the best weights and bias\n",
    "        self.W = best_W\n",
    "        self.b = best_b\n",
    "\n",
    "    def compute_gradient(self, X, y_one_hot, W, b):\n",
    "       \n",
    "        n_samples = X.shape[0] # get  num of inputs\n",
    "        \n",
    "        # Compute predictions\n",
    "        z = np.dot(X, W) + b  # Shape: (n_samples, n_classes)\n",
    "        y_pred = self.softmax(z)\n",
    "        \n",
    "        # Compute gradients\n",
    "        error = y_pred - y_one_hot\n",
    "        dW = np.dot(X.T, error) / n_samples\n",
    "        db = np.sum(error, axis=0) / n_samples\n",
    "        \n",
    "        return dW, db\n",
    "\n",
    "    def softmax(self, z):\n",
    "\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Subtract max for numerical stability\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    def compute_cost(self, X, y_one_hot, W, b):\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Compute predictions\n",
    "        z = np.dot(X, W) + b\n",
    "        y_pred = self.softmax(z)\n",
    "        \n",
    "        # Compute cross-entropy loss\n",
    "        loss = -np.sum(y_one_hot * np.log(y_pred + 1e-15)) / n_samples\n",
    "        return loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        z = np.dot(X, self.W) + self.b\n",
    "        y_pred = self.softmax(z)\n",
    "        return np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Point 0, Iteration 0, Cost: 1.5516518996491795\n",
      "Starting Point 0, Iteration 100, Cost: 1.1357508614058431\n",
      "Starting Point 0, Iteration 200, Cost: 1.1009967161381646\n",
      "Starting Point 0, Iteration 300, Cost: 1.0804510228285107\n",
      "Starting Point 0, Iteration 400, Cost: 1.0684170782111986\n",
      "Starting Point 0, Iteration 500, Cost: 1.0613512391644835\n",
      "Starting Point 0, Iteration 600, Cost: 1.0571586300721088\n",
      "Starting Point 0, Iteration 700, Cost: 1.0546304898249967\n",
      "Starting Point 0, Iteration 800, Cost: 1.0530751551267858\n",
      "Starting Point 0, Iteration 900, Cost: 1.0520963522178368\n",
      "Starting Point 1, Iteration 0, Cost: 1.494476191392832\n",
      "Starting Point 1, Iteration 100, Cost: 1.1159350287535643\n",
      "Starting Point 1, Iteration 200, Cost: 1.0893932161585522\n",
      "Starting Point 1, Iteration 300, Cost: 1.0736510803845964\n",
      "Starting Point 1, Iteration 400, Cost: 1.0643240832837555\n",
      "Starting Point 1, Iteration 500, Cost: 1.058778943052082\n",
      "Starting Point 1, Iteration 600, Cost: 1.0554612090332263\n",
      "Starting Point 1, Iteration 700, Cost: 1.0534585882150214\n",
      "Starting Point 1, Iteration 800, Cost: 1.0522361481450668\n",
      "Starting Point 1, Iteration 900, Cost: 1.0514797116949959\n",
      "Starting Point 2, Iteration 0, Cost: 1.1996266782150151\n",
      "Starting Point 2, Iteration 100, Cost: 1.1428908532761322\n",
      "Starting Point 2, Iteration 200, Cost: 1.1134946154740728\n",
      "Starting Point 2, Iteration 300, Cost: 1.0930095432368732\n",
      "Starting Point 2, Iteration 400, Cost: 1.0790571392352752\n",
      "Starting Point 2, Iteration 500, Cost: 1.0696993458585993\n",
      "Starting Point 2, Iteration 600, Cost: 1.063469854191014\n",
      "Starting Point 2, Iteration 700, Cost: 1.0593233482286648\n",
      "Starting Point 2, Iteration 800, Cost: 1.0565466232356993\n",
      "Starting Point 2, Iteration 900, Cost: 1.054667240256139\n",
      "Predicted class labels: [0 1 0 1 1 0 0 0 0 1 1 2 0 1 0 1 0 2 2 1 0 0 0 2 0 2 0 1 0 2 0 0 1 0 0 0 1\n",
      " 0 2 0 2 0 2 1 0 0 0 0 1 1 1 2 2 1 1 1 0 0 1 0 0 1 2 0 2 0 2 0 1 1 0 0 0 2\n",
      " 0 0 2 0 0 0 2 0 2 1 1 2 1 1 2 1 2 2 2 2 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Example dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 3)  # 100 samples with 3 features\n",
    "y = np.random.randint(0, 3, 100)  # 3 classes (0, 1, 2)\n",
    "\n",
    "# Create and train the model with multiple starting points and momentum\n",
    "model = MultiClassLogisticClassifier(learning_rate=0.1, max_iter=1000, num_starting_points=3, use_momentum=True)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X)\n",
    "print(\"Predicted class labels:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. [10 points] Train and tune a logistic classifier model to predict the digits of the MNIST dataset. During tuning, perform hyperparameter searches with $n$ starting points, momentum, both, and neither. Use a 60% / 20% / 20% for training / validation / testing split. Compare the results in terms of test performance (confusion matrix and classification report), fit runtime, and predict runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "FeedforwardNeuralNetworkSGD.fit() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[176], line 139\u001b[0m\n\u001b[1;32m    135\u001b[0m testY \u001b[38;5;241m=\u001b[39m to_categorical(testY, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    137\u001b[0m mnist_model \u001b[38;5;241m=\u001b[39m FeedforwardNeuralNetworkSGD([\u001b[38;5;241m784\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m10\u001b[39m], \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m--> 139\u001b[0m mnist_model\u001b[38;5;241m.\u001b[39mfit(trainX)\n\u001b[1;32m    140\u001b[0m mnist_model\u001b[38;5;241m.\u001b[39mfit(trainX, trainY, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# print the classification performance\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: FeedforwardNeuralNetworkSGD.fit() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "class FeedforwardNeuralNetworkSGD:\n",
    "    def __init__(self, learning_rate = 0.01, max_iter = 1000, num_starting_points = 1, use_momentum = False, momentum_factor = 0.9, batchSize = 32):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.num_starting_points = num_starting_points\n",
    "        self.use_momentum = use_momentum\n",
    "        self.momentum_factor = momentum_factor\n",
    "        self.batchSize = batchSize\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y)) # get unique input of y's\n",
    "        \n",
    "        # Initialize weights and biases for multiple starting points\n",
    "        self.W = np.random.randn(n_features, n_classes, self.num_starting_points) # must be 3d due to multiple starting points\n",
    "        self.b = np.zeros((n_classes, self.num_starting_points))\n",
    "\n",
    "        # Initialize momentum for gradient descent\n",
    "        if self.use_momentum:\n",
    "            self.velocity_W = np.zeros((n_features, n_classes, self.num_starting_points))\n",
    "            self.velocity_b = np.zeros((n_classes, self.num_starting_points))\n",
    "        \n",
    "        # Convert y to one-hot encoding\n",
    "        y_one_hot = np.eye(n_classes)[y]\n",
    "        \n",
    "        # save the best weight and bias\n",
    "        best_cost = float('inf') # intially save the largest number to save lower costs\n",
    "        best_W = None\n",
    "        best_b = None\n",
    "\n",
    "        # Gradient Descent for each starting points\n",
    "        for start_point in range(self.num_starting_points):\n",
    "            W = self.W[:, :, start_point]\n",
    "            b = self.b[:, start_point]\n",
    "\n",
    "            if self.use_momentum:\n",
    "                velocity_W = self.velocity_W[:, :, start_point]\n",
    "                velocity_b = self.velocity_b[:, start_point]\n",
    "\n",
    "            for iteration in range(self.max_iter):\n",
    "                # Compute gradient\n",
    "                dW, db = self.compute_gradient(X, y_one_hot, W, b)\n",
    "                \n",
    "                if self.use_momentum:\n",
    "                    # Update momentum terms\n",
    "                    velocity_W = self.momentum_factor * velocity_W + (1 - self.momentum_factor) * dW\n",
    "                    velocity_b = self.momentum_factor * velocity_b + (1 - self.momentum_factor) * db\n",
    "                    \n",
    "                    # Update weights and biases with momentum\n",
    "                    W -= self.learning_rate * velocity_W\n",
    "                    b -= self.learning_rate * velocity_b\n",
    "                else:\n",
    "                    # Update weights and biases without momentum\n",
    "                    W -= self.learning_rate * dW\n",
    "                    b -= self.learning_rate * db\n",
    "\n",
    "                # Calculate cost to monitor the training process\n",
    "                if iteration % 100 == 0:\n",
    "                    cost = self.compute_cost(X, y_one_hot, W, b)\n",
    "                    print(f\"Starting Point {start_point}, Iteration {iteration}, Cost: {cost}\")\n",
    "\n",
    "                # Keep track of the best weights and biases based on cost\n",
    "                if cost < best_cost:\n",
    "                    best_cost = cost\n",
    "                    best_W = W\n",
    "                    best_b = b\n",
    "\n",
    "            # Save the best weights and biases for current starting point\n",
    "            self.W[:, :, start_point] = W\n",
    "            self.b[:, start_point] = b\n",
    "        \n",
    "        # Set the best weights and bias\n",
    "        self.W = best_W\n",
    "        self.b = best_b\n",
    "\n",
    "    def compute_gradient(self, X, y_one_hot, W, b):\n",
    "       \n",
    "        n_samples = X.shape[0] # get  num of inputs\n",
    "        \n",
    "        # Compute predictions\n",
    "        z = np.dot(X, W) + b  # Shape: (n_samples, n_classes)\n",
    "        y_pred = self.softmax(z)\n",
    "        \n",
    "        # Compute gradients\n",
    "        error = y_pred - y_one_hot\n",
    "        dW = np.dot(X.T, error) / n_samples\n",
    "        db = np.sum(error, axis=0) / n_samples\n",
    "        \n",
    "        return dW, db\n",
    "\n",
    "    def softmax(self, z):\n",
    "\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Subtract max for numerical stability\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    def compute_cost(self, X, y_one_hot, W, b):\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Compute predictions\n",
    "        z = np.dot(X, W) + b\n",
    "        y_pred = self.softmax(z)\n",
    "        \n",
    "        # Compute cross-entropy loss\n",
    "        loss = -np.sum(y_one_hot * np.log(y_pred + 1e-15)) / n_samples\n",
    "        return loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        z = np.dot(X, self.W) + self.b\n",
    "        y_pred = self.softmax(z)\n",
    "        return np.argmax(y_pred, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#############\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# load the full MNIST dataset: both data and labels\n",
    "((trainX, trainY), (testX, testY)) = mnist.load_data()\n",
    "\n",
    "# scale the data to values in [0,1]\n",
    "trainX = trainX.astype('float32')/255.0\n",
    "testX = testX.astype('float32')/255.0\n",
    "\n",
    "# reshape the data\n",
    "trainX = trainX.reshape([60000, 28*28])\n",
    "testX = testX.reshape([10000, 28*28])\n",
    "\n",
    "# convert the digits to one-hot vectors\n",
    "trainY = to_categorical(trainY, 10)\n",
    "testY = to_categorical(testY, 10)\n",
    "\n",
    "mnist_model = FeedforwardNeuralNetworkSGD([784, 32, 16, 10], 0.5, 32)\n",
    "\n",
    "mnist_model.fit(trainX)\n",
    "mnist_model.fit(trainX, trainY, 100, 1)\n",
    "\n",
    "\n",
    "\n",
    "# couldn't solve due to lack of keras knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
